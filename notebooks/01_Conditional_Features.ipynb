{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7e47d8-4329-439e-be53-3c290b4d16ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/zach/.cache/uv/archive-v0/KzrkVW0XF0lEIphRjw_nJ` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m181 packages\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m165 packages\u001b[0m \u001b[2min 0.10ms\u001b[0m\u001b[0m\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "!uv add sae-lens transformer-lens sae-dashboard pandas plotly tqdm networkx matplotlib seaborn pyvis\n",
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb260a09-99ad-4edf-ad1d-bcef440999ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "{'architecture': 'standard', 'd_in': 768, 'd_sae': 24576, 'activation_fn_str': 'relu', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': False, 'context_size': 128, 'model_name': 'gpt2-small', 'hook_name': 'blocks.7.hook_resid_pre', 'hook_layer': 7, 'hook_head_index': None, 'prepend_bos': True, 'dataset_path': 'Skylion007/openwebtext', 'dataset_trust_remote_code': True, 'normalize_activations': 'none', 'dtype': 'torch.float32', 'device': 'cuda', 'sae_lens_training_version': None, 'activation_fn_kwargs': {}, 'neuronpedia_id': 'gpt2-small/7-res-jb', 'model_from_pretrained_kwargs': {'center_writing_weights': True}, 'seqpos_slice': (None,)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zach/jupyter/.venv/lib/python3.10/site-packages/sae_lens/sae.py:151: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained_no_processing(\"gpt2-small\", device=device)\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience.\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",  # <- Release name\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",  # <- SAE id (not always a hook point!)\n",
    "    device=device,\n",
    ")\n",
    "print(sae.cfg.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d10e0da-3db8-4cfc-b198-41c31bc3062b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens'],\n",
       "    num_rows: 136625\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path=\"NeelNanda/pile-10k\",\n",
    "    split=\"train\",\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=model.tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    max_length=sae.cfg.context_size,\n",
    "    add_bos_token=sae.cfg.prepend_bos,\n",
    ")\n",
    "\n",
    "token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3b941b-9393-4baa-b2ac-415b37f2aa38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>layer</th>\n",
       "      <th>feature</th>\n",
       "      <th>description</th>\n",
       "      <th>explanationModelName</th>\n",
       "      <th>typeName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>218</td>\n",
       "      <td>stars and dashed for censoring expletives</td>\n",
       "      <td>None</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>218</td>\n",
       "      <td>stars and dashes for censoring expletives</td>\n",
       "      <td>None</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>218</td>\n",
       "      <td>offensive language and expletives</td>\n",
       "      <td>None</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>2020</td>\n",
       "      <td>names of people</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>3493</td>\n",
       "      <td>references to nazism</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24568</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>24571</td>\n",
       "      <td>locations and cities paired with information s...</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24569</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>24572</td>\n",
       "      <td>actions related to personal grooming, such as ...</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24570</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>24573</td>\n",
       "      <td>words containing the sequence \"lo\"</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24571</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>24574</td>\n",
       "      <td>instances of added or inserted text</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24572</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>24575</td>\n",
       "      <td>positive aspects or characteristics of a creat...</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24573 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          modelId     layer feature  \\\n",
       "0      gpt2-small  7-res-jb     218   \n",
       "1      gpt2-small  7-res-jb     218   \n",
       "2      gpt2-small  7-res-jb     218   \n",
       "3      gpt2-small  7-res-jb    2020   \n",
       "4      gpt2-small  7-res-jb    3493   \n",
       "...           ...       ...     ...   \n",
       "24568  gpt2-small  7-res-jb   24571   \n",
       "24569  gpt2-small  7-res-jb   24572   \n",
       "24570  gpt2-small  7-res-jb   24573   \n",
       "24571  gpt2-small  7-res-jb   24574   \n",
       "24572  gpt2-small  7-res-jb   24575   \n",
       "\n",
       "                                             description explanationModelName  \\\n",
       "0              stars and dashed for censoring expletives                 None   \n",
       "1              stars and dashes for censoring expletives                 None   \n",
       "2                      offensive language and expletives                 None   \n",
       "3                                        names of people        gpt-3.5-turbo   \n",
       "4                                   references to nazism        gpt-3.5-turbo   \n",
       "...                                                  ...                  ...   \n",
       "24568  locations and cities paired with information s...        gpt-3.5-turbo   \n",
       "24569  actions related to personal grooming, such as ...        gpt-3.5-turbo   \n",
       "24570                 words containing the sequence \"lo\"        gpt-3.5-turbo   \n",
       "24571                instances of added or inserted text        gpt-3.5-turbo   \n",
       "24572  positive aspects or characteristics of a creat...        gpt-3.5-turbo   \n",
       "\n",
       "                 typeName  \n",
       "0      oai_token-act-pair  \n",
       "1      oai_token-act-pair  \n",
       "2      oai_token-act-pair  \n",
       "3      oai_token-act-pair  \n",
       "4      oai_token-act-pair  \n",
       "...                   ...  \n",
       "24568  oai_token-act-pair  \n",
       "24569  oai_token-act-pair  \n",
       "24570  oai_token-act-pair  \n",
       "24571  oai_token-act-pair  \n",
       "24572  oai_token-act-pair  \n",
       "\n",
       "[24573 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.neuronpedia.org/api/explanation/export?modelId=gpt2-small&saeId=7-res-jb\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "# convert to pandas\n",
    "data = response.json()\n",
    "explanations_df = pd.DataFrame(data)\n",
    "# rename index to \"feature\"\n",
    "explanations_df.rename(columns={\"index\": \"feature\"}, inplace=True)\n",
    "# explanations_df[\"feature\"] = explanations_df[\"feature\"].astype(int)\n",
    "explanations_df[\"description\"] = explanations_df[\"description\"].apply(\n",
    "    lambda x: x.lower()\n",
    ")\n",
    "explanations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e014d37e-7129-4c61-8b73-8c48cc03d803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAEs don't reconstruct activation perfectly, so if you attach an SAE and want the model to stay performant, you need to use the error term.\n",
    "# This is because the SAE will be used to modify the forward pass, and if it doesn't reconstruct the activations well, the outputs may be effected.\n",
    "# Good SAEs have small error terms but it's something to be mindful of.\n",
    "\n",
    "sae.use_error_term  # If use error term is set to false, we will modify the forward pass by using the sae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c87ab14-8d08-46ac-9e03-59326e2dfec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zach/jupyter/.venv/lib/python3.10/site-packages/sae_lens/training/activations_store.py:301: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# instantiate an object to hold activations from a dataset\n",
    "from sae_lens import ActivationsStore\n",
    "\n",
    "# a convenient way to instantiate an activation store is to use the from_sae method\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    # fairly conservative parameters here so can use same for larger\n",
    "    # models without running out of memory.\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=2048,\n",
    "    n_batches_in_buffer=16,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "def list_flatten(nested_list):\n",
    "    return [x for y in nested_list for x in y]\n",
    "\n",
    "\n",
    "# A very handy function Neel wrote to get context around a feature activation\n",
    "def make_token_df(tokens, len_prefix=5, len_suffix=3, model=model):\n",
    "    str_tokens = [model.to_str_tokens(t) for t in tokens]\n",
    "    unique_token = [\n",
    "        [f\"{s}/{i}\" for i, s in enumerate(str_tok)] for str_tok in str_tokens\n",
    "    ]\n",
    "\n",
    "    context = []\n",
    "    prompt = []\n",
    "    pos = []\n",
    "    label = []\n",
    "    for b in range(tokens.shape[0]):\n",
    "        for p in range(tokens.shape[1]):\n",
    "            prefix = \"\".join(str_tokens[b][max(0, p - len_prefix) : p])\n",
    "            if p == tokens.shape[1] - 1:\n",
    "                suffix = \"\"\n",
    "            else:\n",
    "                suffix = \"\".join(\n",
    "                    str_tokens[b][p + 1 : min(tokens.shape[1] - 1, p + 1 + len_suffix)]\n",
    "                )\n",
    "            current = str_tokens[b][p]\n",
    "            context.append(f\"{prefix}|{current}|{suffix}\")\n",
    "            prompt.append(b)\n",
    "            pos.append(p)\n",
    "            label.append(f\"{b}/{p}\")\n",
    "    # print(len(batch), len(pos), len(context), len(label))\n",
    "    return pd.DataFrame(\n",
    "        dict(\n",
    "            str_tokens=list_flatten(str_tokens),\n",
    "            unique_token=list_flatten(unique_token),\n",
    "            context=context,\n",
    "            prompt=prompt,\n",
    "            pos=pos,\n",
    "            label=label,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fbf2a99-3123-48bb-ad23-4a9cc48d22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cat(tensors, dim=0):\n",
    "    \"\"\"\n",
    "    Concatenate a list of (dense or sparse) tensors along `dim`, returning\n",
    "    a single sparse_coo_tensor.  Never builds a full dense intermediate.\n",
    "    \"\"\"\n",
    "    # device / dtype from first\n",
    "    device = tensors[0].device\n",
    "    dtype  = tensors[0].dtype\n",
    "    \n",
    "    indices_list = []\n",
    "    values_list  = []\n",
    "    offset = 0\n",
    "    \n",
    "    for t in tensors:\n",
    "        # 1) Make sparse COO\n",
    "        sp = t.to_sparse()         # if t already sparse, this is essentially a no-op\n",
    "        \n",
    "        idx = sp.indices().clone() # shape: (ndim, nnz_i)\n",
    "        vals = sp.values()         # shape: (nnz_i, ...)\n",
    "        \n",
    "        # 2) shift the cat‐axis\n",
    "        idx[dim] += offset\n",
    "        \n",
    "        indices_list.append(idx)\n",
    "        values_list.append(vals)\n",
    "        \n",
    "        # 3) bump offset for next chunk\n",
    "        offset += t.size(dim)\n",
    "    \n",
    "    # 4) stitch them together\n",
    "    all_indices = torch.cat(indices_list, dim=1)  # (ndim, total_nnz)\n",
    "    all_values  = torch.cat(values_list,  dim=0)  # (total_nnz, ...)\n",
    "    \n",
    "    # 5) figure out new size\n",
    "    out_size = list(tensors[0].shape)\n",
    "    out_size[dim] = offset\n",
    "    \n",
    "    # 6) build final sparse tensor\n",
    "    out = torch.sparse_coo_tensor(all_indices, all_values, out_size,\n",
    "                                  dtype=dtype, device=device)\n",
    "    return out.coalesce()  # optionally coalesce duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "def7cba6-aa9f-4a51-8d32-a37a29b1f576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sae.W_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e3334-85dc-4db6-8f84-1ad3f438906b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                               | 0/4096 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Examples found: 246521:   6%| | 243/4096 [00:55<15:05,  4.26i"
     ]
    }
   ],
   "source": [
    "total_batches = 4096\n",
    "feature_list = torch.randint(0, sae.cfg.d_sae, (total_batches,))\n",
    "examples_found = 0\n",
    "all_fired_tokens = []\n",
    "all_feature_acts = []\n",
    "all_reconstructions = []\n",
    "all_token_dfs = []\n",
    "\n",
    "# Original implementation from Nanda would balloon CPU usage, detaching fixes that.\n",
    "# Updated implementation is only bounded by model + token overhead, I think \n",
    "# Only hits 3Gb of VRAM on GPT2 regardless of batch size.\n",
    "# TODO - Scale on up to full SAE size\n",
    "batch_size_prompts = activation_store.store_batch_size_prompts\n",
    "batch_size_tokens = activation_store.context_size * batch_size_prompts\n",
    "pbar = tqdm(range(total_batches))\n",
    "for i in pbar:\n",
    "    tokens = activation_store.get_batch_tokens()\n",
    "    tokens_df = make_token_df(tokens)\n",
    "    tokens_df[\"batch\"] = i\n",
    "\n",
    "    flat_tokens = tokens.flatten()\n",
    "\n",
    "    _, cache = model.run_with_cache(\n",
    "        tokens, stop_at_layer=sae.cfg.hook_layer + 1, names_filter=[sae.cfg.hook_name]\n",
    "    )\n",
    "    sae_in = cache[sae.cfg.hook_name]\n",
    "    feature_acts = sae.encode(sae_in).squeeze()\n",
    "\n",
    "    feature_acts = feature_acts.flatten(0, 1)\n",
    "    fired_mask = (feature_acts[:, feature_list]).sum(dim=-1) > 0\n",
    "    fired_tokens = model.to_str_tokens(flat_tokens[fired_mask])\n",
    "    reconstruction = feature_acts[fired_mask][:, feature_list] @ sae.W_dec[feature_list]\n",
    "\n",
    "    token_df = tokens_df.iloc[fired_mask.cpu().nonzero().flatten().numpy()]\n",
    "    #all_token_dfs.append(token_df)\n",
    "    all_feature_acts.append(feature_acts[fired_mask][:, feature_list].cpu().detach())\n",
    "    #all_fired_tokens.append(fired_tokens)\n",
    "    #all_reconstructions.append(reconstruction)\n",
    "\n",
    "    examples_found += len(fired_tokens)\n",
    "    # print(f\"Examples found: {examples_found}\")\n",
    "    # update description\n",
    "    pbar.set_description(f\"Examples found: {examples_found}\")\n",
    "\n",
    "# flatten the list of lists\n",
    "#all_token_dfs = pd.concat(all_token_dfs)\n",
    "#all_fired_tokens = list_flatten(all_fired_tokens)\n",
    "#all_reconstructions = torch.cat(all_reconstructions)\n",
    "all_feature_acts = sparse_cat(all_feature_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddac016b-d93a-42b9-bb65-a144986d4e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_20622</th>\n",
       "      <th>feature_533</th>\n",
       "      <th>feature_4239</th>\n",
       "      <th>feature_22677</th>\n",
       "      <th>feature_3417</th>\n",
       "      <th>feature_5544</th>\n",
       "      <th>feature_1407</th>\n",
       "      <th>feature_15424</th>\n",
       "      <th>feature_9833</th>\n",
       "      <th>feature_11899</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_2945</th>\n",
       "      <th>feature_1371</th>\n",
       "      <th>feature_5991</th>\n",
       "      <th>feature_14771</th>\n",
       "      <th>feature_12620</th>\n",
       "      <th>feature_7546</th>\n",
       "      <th>feature_24393</th>\n",
       "      <th>feature_16096</th>\n",
       "      <th>feature_15380</th>\n",
       "      <th>feature_10788</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>222.595535</td>\n",
       "      <td>253.000076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.669052</td>\n",
       "      <td>154.762756</td>\n",
       "      <td>229.317795</td>\n",
       "      <td>244.764374</td>\n",
       "      <td>...</td>\n",
       "      <td>213.096619</td>\n",
       "      <td>76.259781</td>\n",
       "      <td>224.75563</td>\n",
       "      <td>228.64592</td>\n",
       "      <td>254.42923</td>\n",
       "      <td>50.387886</td>\n",
       "      <td>229.89415</td>\n",
       "      <td>101.386383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.366203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.931584</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.003434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31298</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31299</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31301</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31302</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31303 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature_20622  feature_533  feature_4239  feature_22677  feature_3417  \\\n",
       "0                0.0   222.595535    253.000076            0.0           0.0   \n",
       "1                0.0     0.000000      0.000000            0.0           0.0   \n",
       "2                0.0     0.000000      0.000000            0.0           0.0   \n",
       "3                0.0     0.000000      0.000000            0.0           0.0   \n",
       "4                0.0     0.000000      0.000000            0.0           0.0   \n",
       "...              ...          ...           ...            ...           ...   \n",
       "31298            0.0     0.000000      0.000000            0.0           0.0   \n",
       "31299            0.0     0.000000      0.000000            0.0           0.0   \n",
       "31300            0.0     0.000000      0.000000            0.0           0.0   \n",
       "31301            0.0     0.000000      0.000000            0.0           0.0   \n",
       "31302            0.0     0.000000      0.000000            0.0           0.0   \n",
       "\n",
       "       feature_5544  feature_1407  feature_15424  feature_9833  feature_11899  \\\n",
       "0          0.000000    110.669052     154.762756    229.317795     244.764374   \n",
       "1          0.000000      0.000000       0.000000      0.000000       0.000000   \n",
       "2          0.000000      0.000000       0.000000      0.000000       0.000000   \n",
       "3          0.000000      0.000000       0.000000      0.000000       0.000000   \n",
       "4          1.003434      0.000000       0.000000      0.000000       0.000000   \n",
       "...             ...           ...            ...           ...            ...   \n",
       "31298      0.000000      0.000000       0.000000      0.000000       0.000000   \n",
       "31299      0.000000      0.000000       0.000000      0.000000       0.000000   \n",
       "31300      0.000000      0.000000       0.000000      0.000000       0.000000   \n",
       "31301      0.000000      0.000000       0.000000      0.000000       0.000000   \n",
       "31302      0.000000      0.000000       0.000000      0.000000       0.000000   \n",
       "\n",
       "       ...  feature_2945  feature_1371  feature_5991  feature_14771  \\\n",
       "0      ...    213.096619     76.259781     224.75563      228.64592   \n",
       "1      ...      0.000000      0.000000       0.00000        0.00000   \n",
       "2      ...      0.000000      0.931584       0.00000        0.00000   \n",
       "3      ...      0.000000      0.000000       0.00000        0.00000   \n",
       "4      ...      0.000000      0.000000       0.00000        0.00000   \n",
       "...    ...           ...           ...           ...            ...   \n",
       "31298  ...      0.000000      0.000000       0.00000        0.00000   \n",
       "31299  ...      0.000000      0.000000       0.00000        0.00000   \n",
       "31300  ...      0.000000      0.000000       0.00000        0.00000   \n",
       "31301  ...      0.000000      0.000000       0.00000        0.00000   \n",
       "31302  ...      0.000000      0.000000       0.00000        0.00000   \n",
       "\n",
       "       feature_12620  feature_7546  feature_24393  feature_16096  \\\n",
       "0          254.42923     50.387886      229.89415     101.386383   \n",
       "1            0.00000      0.000000        0.00000       0.000000   \n",
       "2            0.00000      0.000000        0.00000       0.000000   \n",
       "3            0.00000      0.000000        0.00000       0.000000   \n",
       "4            0.00000      0.000000        0.00000       0.000000   \n",
       "...              ...           ...            ...            ...   \n",
       "31298        0.00000      0.000000        0.00000       0.000000   \n",
       "31299        0.00000      0.000000        0.00000       0.000000   \n",
       "31300        0.00000      0.000000        0.00000       0.000000   \n",
       "31301        0.00000      0.000000        0.00000       0.000000   \n",
       "31302        0.00000      0.000000        0.00000       0.000000   \n",
       "\n",
       "       feature_15380  feature_10788  \n",
       "0                0.0     124.366203  \n",
       "1                0.0       0.000000  \n",
       "2                0.0       0.000000  \n",
       "3                0.0       0.000000  \n",
       "4                0.0       0.000000  \n",
       "...              ...            ...  \n",
       "31298            0.0       0.000000  \n",
       "31299            0.0       0.000000  \n",
       "31300            0.0       0.000000  \n",
       "31301            0.0       0.000000  \n",
       "31302            0.0       0.000000  \n",
       "\n",
       "[31303 rows x 100 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_acts_df = pd.DataFrame(\n",
    "    all_feature_acts.numpy(),\n",
    "    columns=[f\"feature_{i}\" for i in feature_list],\n",
    ")\n",
    "feature_acts_df.shape\n",
    "feature_acts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f1b33bc-8acd-4c02-9f4c-5694e7af0827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zach/jupyter/.venv/lib/python3.10/site-packages/sklearn/covariance/_graph_lasso.py:168: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[indices != idx, idx] = -precision_[idx, idx] * coefs\n",
      "/home/zach/jupyter/.venv/lib/python3.10/site-packages/sklearn/covariance/_graph_lasso.py:169: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[idx, indices != idx] = -precision_[idx, idx] * coefs\n",
      "/home/zach/jupyter/.venv/lib/python3.10/site-packages/sklearn/covariance/_graph_lasso.py:168: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[indices != idx, idx] = -precision_[idx, idx] * coefs\n",
      "/home/zach/jupyter/.venv/lib/python3.10/site-packages/sklearn/covariance/_graph_lasso.py:169: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[idx, indices != idx] = -precision_[idx, idx] * coefs\n",
      "/home/zach/jupyter/.venv/lib/python3.10/site-packages/sklearn/covariance/_graph_lasso.py:168: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[indices != idx, idx] = -precision_[idx, idx] * coefs\n",
      "/home/zach/jupyter/.venv/lib/python3.10/site-packages/sklearn/covariance/_graph_lasso.py:169: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[idx, indices != idx] = -precision_[idx, idx] * coefs\n",
      "/home/zach/jupyter/.venv/lib/python3.10/site-packages/numpy/core/_methods.py:173: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      "/home/zach/jupyter/.venv/lib/python3.10/site-packages/sklearn/covariance/_graph_lasso.py:192: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.176e-04\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.covariance import GraphicalLassoCV\n",
    "gl = GraphicalLassoCV(cv=3, max_iter=100).fit(all_feature_acts.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db64560e-3432-48cf-9630-ef66863561a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.81346212e+01,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  9.88045507e-04, -0.00000000e+00, ...,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00],\n",
       "       [ 0.00000000e+00, -0.00000000e+00,  8.45122338e-04, ...,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00],\n",
       "       ...,\n",
       "       [ 0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
       "         3.90858192e-03,  0.00000000e+00, -0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  7.01820370e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
       "        -0.00000000e+00,  0.00000000e+00,  2.59431815e-03]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gl.precision_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2c5018d-01a8-4577-8644-4ef6047999ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_to_graph(precision_matrix, feature_names=None, threshold=0.1, trim_names_len=10):\n",
    "    \"\"\"Convert precision matrix to networkx graph with feature names\"\"\"\n",
    "    \n",
    "    # threshold to keep only strong connections\n",
    "    adj_matrix = np.abs(precision_matrix) > threshold\n",
    "    np.fill_diagonal(adj_matrix, False)  # remove self-loops\n",
    "    \n",
    "    # create networkx graph\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    \n",
    "    # add feature names as node attributes\n",
    "    if feature_names is not None:\n",
    "        print('Setting feature names')\n",
    "        assert len(feature_names) == precision_matrix.shape[0], \\\n",
    "            f\"Expected {precision_matrix.shape[0]} feature names, got {len(feature_names)}\"\n",
    "        \n",
    "        # add names to nodes\n",
    "        for i, name in enumerate(feature_names):\n",
    "            if i in G.nodes():  # only add if node exists (has edges)\n",
    "                G.nodes[i]['title'] = name[:trim_names_len] + '...' if len(name) > trim_names_len else name\n",
    "                G.nodes[i]['label'] = name\n",
    "    \n",
    "    # add edge weights for visualization\n",
    "    for i, j in G.edges():\n",
    "        G[i][j]['weight'] = abs(precision_matrix[i, j]),\n",
    "        G[i][j]['rel_weight'] = abs(precision_matrix[i, j]) / np.min(precision_matrix[~np.isclose(precision_matrix, 0)])    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d81a02f2-98c3-4e25-9511-f243265ffb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting feature names\n"
     ]
    }
   ],
   "source": [
    "# grab our labels\n",
    "idxs = [int(x.lstrip('feature_').strip()) for x in feature_acts_df.columns]\n",
    "labels = list(explanations_df.loc[idxs, 'description'])\n",
    "G = precision_to_graph(np.abs(gl.precision_), feature_names=labels, threshold=1e-5)\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "#nx.draw(G)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d574fca-c20f-4b7f-9294-de2d5945072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n"
     ]
    }
   ],
   "source": [
    "# This graph is hard to read... but we can work with it still\n",
    "nt = Network('750px', '750px', notebook=True)\n",
    "nt.from_nx(G, show_edge_weights=False)\n",
    "nt.toggle_physics(False)\n",
    "#nt.show('nt.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a76ee01-2dae-4237-bcef-f569129fa7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge 0, Relative Weigth 1499.03:\n",
      "\t- names and terms related to a popular science fiction tv show\n",
      "\t- phrases related to fox news channel\n",
      "Edge 1, Relative Weigth 1493.27:\n",
      "\t- phrases related to considering or predicting potential outcomes or results of actions\n",
      "\t- phrases related to fox news channel\n",
      "Edge 2, Relative Weigth 1484.74:\n",
      "\t- questions and alternatives\n",
      "\t- phrases related to fox news channel\n",
      "Edge 3, Relative Weigth 1483.95:\n",
      "\t- mentions of catastrophic events or overwhelming situations\n",
      "\t- phrases related to fox news channel\n",
      "Edge 4, Relative Weigth 1483.57:\n",
      "\t- mentions of the word \"pokémon\"\n",
      "\t- phrases related to fox news channel\n",
      "Edge 5, Relative Weigth 1482.30:\n",
      "\t- words and phrases that end in \"ly.\"\n",
      "\t- phrases related to fox news channel\n",
      "Edge 6, Relative Weigth 1474.62:\n",
      "\t- phrases related to fox news channel\n",
      "\t- specific paired characters that signify a specific programming construct or syntax\n",
      "Edge 7, Relative Weigth 1474.20:\n",
      "\t- the word \"si\" in various contexts\n",
      "\t- phrases related to fox news channel\n",
      "Edge 8, Relative Weigth 1472.66:\n",
      "\t- technical instructions or explanations\n",
      "\t- phrases related to fox news channel\n",
      "Edge 9, Relative Weigth 1467.92:\n",
      "\t- phrases related to belief, faith, or conviction\n",
      "\t- phrases related to fox news channel\n",
      "Edge 10, Relative Weigth 1465.09:\n",
      "\t- financial incentives or support related to specific groups or actions\n",
      "\t- phrases related to fox news channel\n",
      "Edge 11, Relative Weigth 1461.88:\n",
      "\t- instructions or recommendations in text\n",
      "\t- phrases related to fox news channel\n",
      "Edge 12, Relative Weigth 1461.15:\n",
      "\t- phrases indicating a narrative shift or continuation in a story\n",
      "\t- phrases related to fox news channel\n",
      "Edge 13, Relative Weigth 1460.22:\n",
      "\t- adjectives or verbs related to the concept of 'broad'\n",
      "\t- phrases related to fox news channel\n",
      "Edge 14, Relative Weigth 1455.97:\n",
      "\t- references to the name \"turner.\"\n",
      "\t- phrases related to fox news channel\n",
      "Edge 15, Relative Weigth 1454.35:\n",
      "\t- shapes such as rectangles, triangles, circles, and ellipses\n",
      "\t- phrases related to fox news channel\n",
      "Edge 16, Relative Weigth 1450.49:\n",
      "\t- words starting with the letter 'm' and ending with lower activation words accompanying the 'm' word\n",
      "\t- phrases related to fox news channel\n",
      "Edge 17, Relative Weigth 1449.41:\n",
      "\t- terms related to sports defense, particularly focusing on individual players or team performance\n",
      "\t- phrases related to fox news channel\n",
      "Edge 18, Relative Weigth 1449.30:\n",
      "\t- technical terms related to image processing or coding\n",
      "\t- phrases related to fox news channel\n",
      "Edge 19, Relative Weigth 1448.20:\n",
      "\t- words containing the substring \"vert\" or \"verted\"\n",
      "\t- phrases related to fox news channel\n",
      "Edge 20, Relative Weigth 1444.20:\n",
      "\t- phrases related to fox news channel\n",
      "\t- steps or instructions related to software development and coding\n",
      "Edge 21, Relative Weigth 1440.20:\n",
      "\t- proper nouns related to video games\n",
      "\t- phrases related to fox news channel\n",
      "Edge 22, Relative Weigth 1435.84:\n",
      "\t- words related to technology and projects, potentially involving proper nouns\n",
      "\t- phrases related to fox news channel\n",
      "Edge 23, Relative Weigth 1435.67:\n",
      "\t- phrases related to fox news channel\n",
      "\t- phrases related to regulations and restrictions on different topics such as education, health, and behavior\n",
      "Edge 24, Relative Weigth 1434.15:\n",
      "\t- words related to fishing\n",
      "\t- phrases related to fox news channel\n",
      "Edge 25, Relative Weigth 1434.13:\n",
      "\t- words related to activities and interactions involving people\n",
      "\t- phrases related to fox news channel\n",
      "Edge 26, Relative Weigth 1432.02:\n",
      "\t- phrases related to fox news channel\n",
      "\t- mentions of the name \"roberts\" with varying levels of activation\n",
      "Edge 27, Relative Weigth 1429.44:\n",
      "\t- references or mentions of the green bay packers\n",
      "\t- phrases related to fox news channel\n",
      "Edge 28, Relative Weigth 1421.67:\n",
      "\t- phrases related to fox news channel\n",
      "\t- phrases related to energy production and economics\n",
      "Edge 29, Relative Weigth 1413.55:\n",
      "\t- adjectives and nouns related to negative or disturbing events or characteristics\n",
      "\t- phrases related to fox news channel\n",
      "Edge 30, Relative Weigth 1413.39:\n",
      "\t- mentions of the name \"hadley.\"\n",
      "\t- phrases related to fox news channel\n",
      "Edge 31, Relative Weigth 1371.17:\n",
      "\t- unintelligible or garbled text within the document\n",
      "\t- phrases related to fox news channel\n",
      "Edge 32, Relative Weigth 1360.18:\n",
      "\t- mentions of the country \"north korea.\"\n",
      "\t- phrases related to fox news channel\n",
      "Edge 33, Relative Weigth 1354.01:\n",
      "\t- phrases related to emphasizing a main idea or argument\n",
      "\t- phrases related to fox news channel\n",
      "Edge 34, Relative Weigth 1353.12:\n",
      "\t- phrases related to fox news channel\n",
      "\t- phrases about learning, lessons, and acquiring new knowledge\n",
      "Edge 35, Relative Weigth 1342.11:\n",
      "\t- terms related to the internet service google\n",
      "\t- phrases related to fox news channel\n",
      "Edge 36, Relative Weigth 1329.16:\n",
      "\t- terms related to astronomy and astronomers\n",
      "\t- phrases related to fox news channel\n",
      "Edge 37, Relative Weigth 1306.75:\n",
      "\t- personal pronouns followed by verbs related to actions or states\n",
      "\t- phrases related to fox news channel\n",
      "Edge 38, Relative Weigth 1305.48:\n",
      "\t- phrases related to potential risks or dangers\n",
      "\t- phrases related to fox news channel\n",
      "Edge 39, Relative Weigth 1302.64:\n",
      "\t- names of individuals\n",
      "\t- phrases related to fox news channel\n",
      "Edge 40, Relative Weigth 1290.05:\n",
      "\t- locations or organizations relating to johns hopkins university\n",
      "\t- phrases related to fox news channel\n",
      "Edge 41, Relative Weigth 1270.73:\n",
      "\t- phrases related to fox news channel\n",
      "\t- phrases related to business operations and procedures\n",
      "Edge 42, Relative Weigth 1270.57:\n",
      "\t- phrases related to business operations and procedures\n",
      "\t- phrases related to fox news channel\n",
      "Edge 43, Relative Weigth 1205.08:\n",
      "\t-  phrases related to sources or individuals who wish to remain anonymous\n",
      "\t- phrases related to fox news channel\n",
      "Edge 44, Relative Weigth 1196.30:\n",
      "\t- topics related to crime, including sexual crimes, fraud, and corruption\n",
      "\t- phrases related to fox news channel\n",
      "Edge 45, Relative Weigth 1135.53:\n",
      "\t- phrases related to health and physical condition\n",
      "\t- phrases related to fox news channel\n",
      "Edge 46, Relative Weigth 1125.18:\n",
      "\t- mentions of \"teams\"\n",
      "\t- phrases related to fox news channel\n",
      "Edge 47, Relative Weigth 1073.88:\n",
      "\t- the word or part of the word \"momentum\"\n",
      "\t- phrases related to fox news channel\n",
      "Edge 48, Relative Weigth 990.58:\n",
      "\t- mentions of legal or ownership claims in text\n",
      "\t- phrases related to fox news channel\n",
      "Edge 49, Relative Weigth 876.03:\n",
      "\t- references to specific locations, specifically hometowns\n",
      "\t- phrases related to fox news channel\n",
      "Edge 50, Relative Weigth 859.72:\n",
      "\t- phrases prompting action, specifically related to subscribing or notifications\n",
      "\t- phrases related to fox news channel\n",
      "Edge 51, Relative Weigth 651.23:\n",
      "\t- timestamps and publication information\n",
      "\t- phrases related to fox news channel\n",
      "Edge 52, Relative Weigth 588.84:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- phrases related to business operations and procedures\n",
      "Edge 53, Relative Weigth 588.81:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- phrases related to business operations and procedures\n",
      "Edge 54, Relative Weigth 588.19:\n",
      "\t- locations or organizations relating to johns hopkins university\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "Edge 55, Relative Weigth 583.73:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- terms related to astronomy and astronomers\n",
      "Edge 56, Relative Weigth 582.51:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- terms related to the internet service google\n",
      "Edge 57, Relative Weigth 577.28:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- phrases related to health and physical condition\n",
      "Edge 58, Relative Weigth 574.56:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- mentions of the country \"north korea.\"\n",
      "Edge 59, Relative Weigth 568.34:\n",
      "\t- unintelligible or garbled text within the document\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "Edge 60, Relative Weigth 567.62:\n",
      "\t- the word or part of the word \"momentum\"\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "Edge 61, Relative Weigth 540.31:\n",
      "\t- mentions of the name \"hadley.\"\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "Edge 62, Relative Weigth 540.20:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- adjectives and nouns related to negative or disturbing events or characteristics\n",
      "Edge 63, Relative Weigth 527.35:\n",
      "\t- quotes or reported speech\n",
      "\t- the word or part of the word \"momentum\"\n",
      "Edge 64, Relative Weigth 525.89:\n",
      "\t- quotes or reported speech\n",
      "\t- phrases related to health and physical condition\n",
      "Edge 65, Relative Weigth 524.64:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- phrases related to energy production and economics\n",
      "Edge 66, Relative Weigth 510.43:\n",
      "\t- quotes or reported speech\n",
      "\t- phrases related to business operations and procedures\n",
      "Edge 67, Relative Weigth 510.43:\n",
      "\t- quotes or reported speech\n",
      "\t- phrases related to business operations and procedures\n",
      "Edge 68, Relative Weigth 504.94:\n",
      "\t- locations or organizations relating to johns hopkins university\n",
      "\t- quotes or reported speech\n",
      "Edge 69, Relative Weigth 489.60:\n",
      "\t- quotes or reported speech\n",
      "\t- terms related to astronomy and astronomers\n",
      "Edge 70, Relative Weigth 484.74:\n",
      "\t- quotes or reported speech\n",
      "\t- terms related to the internet service google\n",
      "Edge 71, Relative Weigth 477.51:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- mentions of the name \"roberts\" with varying levels of activation\n",
      "Edge 72, Relative Weigth 473.50:\n",
      "\t- words related to fishing\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "Edge 73, Relative Weigth 469.68:\n",
      "\t- quotes or reported speech\n",
      "\t- mentions of the country \"north korea.\"\n",
      "Edge 74, Relative Weigth 468.21:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- phrases related to regulations and restrictions on different topics such as education, health, and behavior\n",
      "Edge 75, Relative Weigth 467.11:\n",
      "\t- personal reflections or opinions expressed through language\n",
      "\t- phrases related to fox news channel\n",
      "Edge 76, Relative Weigth 466.67:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- words related to technology and projects, potentially involving proper nouns\n",
      "Edge 77, Relative Weigth 458.79:\n",
      "\t- unintelligible or garbled text within the document\n",
      "\t- quotes or reported speech\n",
      "Edge 78, Relative Weigth 450.30:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- proper nouns related to video games\n",
      "Edge 79, Relative Weigth 436.65:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- steps or instructions related to software development and coding\n",
      "Edge 80, Relative Weigth 421.09:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- words containing the substring \"vert\" or \"verted\"\n",
      "Edge 81, Relative Weigth 418.59:\n",
      "\t- technical terms related to image processing or coding\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "Edge 82, Relative Weigth 416.87:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- terms related to sports defense, particularly focusing on individual players or team performance\n",
      "Edge 83, Relative Weigth 412.64:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- words starting with the letter 'm' and ending with lower activation words accompanying the 'm' word\n",
      "Edge 84, Relative Weigth 410.57:\n",
      "\t- quotes or reported speech\n",
      "\t- mentions of the name \"hadley.\"\n",
      "Edge 85, Relative Weigth 410.21:\n",
      "\t- quotes or reported speech\n",
      "\t- adjectives and nouns related to negative or disturbing events or characteristics\n",
      "Edge 86, Relative Weigth 400.32:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- shapes such as rectangles, triangles, circles, and ellipses\n",
      "Edge 87, Relative Weigth 387.21:\n",
      "\t- quotes or reported speech\n",
      "\t- phrases related to energy production and economics\n",
      "Edge 88, Relative Weigth 379.70:\n",
      "\t- adjectives or verbs related to the concept of 'broad'\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "Edge 89, Relative Weigth 375.04:\n",
      "\t- instructions or recommendations in text\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "Edge 90, Relative Weigth 339.20:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- technical instructions or explanations\n",
      "Edge 91, Relative Weigth 333.49:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- specific paired characters that signify a specific programming construct or syntax\n",
      "Edge 92, Relative Weigth 331.90:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- the word \"si\" in various contexts\n",
      "Edge 93, Relative Weigth 320.17:\n",
      "\t- quotes or reported speech\n",
      "\t- mentions of the name \"roberts\" with varying levels of activation\n",
      "Edge 94, Relative Weigth 314.11:\n",
      "\t- words related to fishing\n",
      "\t- quotes or reported speech\n",
      "Edge 95, Relative Weigth 308.23:\n",
      "\t- words and phrases that end in \"ly.\"\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "Edge 96, Relative Weigth 306.58:\n",
      "\t- quotes or reported speech\n",
      "\t- phrases related to regulations and restrictions on different topics such as education, health, and behavior\n",
      "Edge 97, Relative Weigth 304.34:\n",
      "\t- quotes or reported speech\n",
      "\t- words related to technology and projects, potentially involving proper nouns\n",
      "Edge 98, Relative Weigth 303.62:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- mentions of catastrophic events or overwhelming situations\n",
      "Edge 99, Relative Weigth 280.77:\n",
      "\t- quotes or reported speech\n",
      "\t- proper nouns related to video games\n",
      "Edge 100, Relative Weigth 275.94:\n",
      "\t- the word or part of the word \"momentum\"\n",
      "\t- phrases related to health and physical condition\n",
      "Edge 101, Relative Weigth 275.54:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- phrases related to considering or predicting potential outcomes or results of actions\n",
      "Edge 102, Relative Weigth 261.33:\n",
      "\t- quotes or reported speech\n",
      "\t- steps or instructions related to software development and coding\n",
      "Edge 103, Relative Weigth 251.07:\n",
      "\t- names and terms related to a popular science fiction tv show\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "Edge 104, Relative Weigth 238.88:\n",
      "\t- quotes or reported speech\n",
      "\t- words containing the substring \"vert\" or \"verted\"\n",
      "Edge 105, Relative Weigth 235.33:\n",
      "\t- technical terms related to image processing or coding\n",
      "\t- quotes or reported speech\n",
      "Edge 106, Relative Weigth 232.83:\n",
      "\t- quotes or reported speech\n",
      "\t- terms related to sports defense, particularly focusing on individual players or team performance\n",
      "Edge 107, Relative Weigth 226.79:\n",
      "\t- quotes or reported speech\n",
      "\t- words starting with the letter 'm' and ending with lower activation words accompanying the 'm' word\n",
      "Edge 108, Relative Weigth 209.04:\n",
      "\t- quotes or reported speech\n",
      "\t- shapes such as rectangles, triangles, circles, and ellipses\n",
      "Edge 109, Relative Weigth 204.16:\n",
      "\t- the word or part of the word \"momentum\"\n",
      "\t- phrases related to business operations and procedures\n",
      "Edge 110, Relative Weigth 204.15:\n",
      "\t- the word or part of the word \"momentum\"\n",
      "\t- phrases related to business operations and procedures\n",
      "Edge 111, Relative Weigth 189.46:\n",
      "\t- locations or organizations relating to johns hopkins university\n",
      "\t- the word or part of the word \"momentum\"\n",
      "Edge 112, Relative Weigth 179.45:\n",
      "\t- adjectives or verbs related to the concept of 'broad'\n",
      "\t- quotes or reported speech\n",
      "Edge 113, Relative Weigth 176.02:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- questions and alternatives\n",
      "Edge 114, Relative Weigth 172.68:\n",
      "\t- instructions or recommendations in text\n",
      "\t- quotes or reported speech\n",
      "Edge 115, Relative Weigth 171.19:\n",
      "\t- phrases related to health and physical condition\n",
      "\t- phrases related to business operations and procedures\n",
      "Edge 116, Relative Weigth 171.16:\n",
      "\t- phrases related to health and physical condition\n",
      "\t- phrases related to business operations and procedures\n",
      "Edge 117, Relative Weigth 167.36:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- mentions of the word \"pokémon\"\n",
      "Edge 118, Relative Weigth 155.64:\n",
      "\t- locations or organizations relating to johns hopkins university\n",
      "\t- phrases related to health and physical condition\n",
      "Edge 119, Relative Weigth 154.02:\n",
      "\t- the word or part of the word \"momentum\"\n",
      "\t- terms related to astronomy and astronomers\n",
      "Edge 120, Relative Weigth 143.22:\n",
      "\t- phrases related to hierarchical levels or categories (e.g., tiers)\n",
      "\t- phrases related to fox news channel\n",
      "Edge 121, Relative Weigth 142.58:\n",
      "\t- the word or part of the word \"momentum\"\n",
      "\t- terms related to the internet service google\n",
      "Edge 122, Relative Weigth 120.95:\n",
      "\t- quotes or reported speech\n",
      "\t- technical instructions or explanations\n",
      "Edge 123, Relative Weigth 118.30:\n",
      "\t- phrases related to health and physical condition\n",
      "\t- terms related to astronomy and astronomers\n",
      "Edge 124, Relative Weigth 115.55:\n",
      "\t- the word or part of the word \"momentum\"\n",
      "\t- mentions of the country \"north korea.\"\n",
      "Edge 125, Relative Weigth 112.85:\n",
      "\t- quotes or reported speech\n",
      "\t- specific paired characters that signify a specific programming construct or syntax\n",
      "Edge 126, Relative Weigth 110.67:\n",
      "\t- quotes or reported speech\n",
      "\t- the word \"si\" in various contexts\n",
      "Edge 127, Relative Weigth 106.26:\n",
      "\t- terms related to the internet service google\n",
      "\t- phrases related to health and physical condition\n",
      "Edge 128, Relative Weigth 96.81:\n",
      "\t- unintelligible or garbled text within the document\n",
      "\t- the word or part of the word \"momentum\"\n",
      "Edge 129, Relative Weigth 83.21:\n",
      "\t- phrases related to business operations and procedures\n",
      "\t- phrases related to business operations and procedures\n",
      "Edge 130, Relative Weigth 81.86:\n",
      "\t- phrases related to statements or opinions expressed by someone\n",
      "\t- phrases related to belief, faith, or conviction\n",
      "Edge 131, Relative Weigth 78.18:\n",
      "\t- mentions of the country \"north korea.\"\n",
      "\t- phrases related to health and physical condition\n",
      "Edge 132, Relative Weigth 76.46:\n",
      "\t- quotes or reported speech\n",
      "\t- words and phrases that end in \"ly.\"\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the strongest edges\n",
    "def print_edge_labels(idx, edge):\n",
    "    \"\"\"Print edge labels based on SAE features + weight. Utility printing function\"\"\"\n",
    "    i, j, d = edge\n",
    "    rel_weight = d['rel_weight']\n",
    "    print(f\"Edge {idx}, Relative Weigth {rel_weight:.2f}:\\n\\t- {labels[i]}\\n\\t- {labels[j]}\")\n",
    "\n",
    "ordered_edges=sorted(G.edges(data=True), key=lambda edge: -edge[2].get('rel_weight', 1))\n",
    "for idx, edge in enumerate(ordered_edges):\n",
    "    print_edge_labels(idx, edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c2646-584c-465a-929e-56fcf7e17537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a05a298-d822-4d17-8066-011eb769fa40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
